{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzfJfOi8WRJp"
      },
      "source": [
        "# üìù Pattern Recognition & Biometrics. TP Traitement du Langage Naturel ou *NLP (Natural Language Processing)*\n",
        "\n",
        "Par Omar Galarraga et Sonia Garcia"
      ],
      "id": "DzfJfOi8WRJp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWDOsRaVWRJx"
      },
      "source": [
        "Dans ce travail pratique, nous constituerons un *pipeline* de traitement du langage naturel avec diff√©rents mod√®les pour analyser le chapitre I du roman \"Le Petit Prince\" d'Antoine de Saint-Exup√©ry"
      ],
      "id": "EWDOsRaVWRJx"
    },
    {
      "cell_type": "code",
      "source": [
        "#Modifiez votre chemin d'acc√®s si besoin\n",
        "with open('lepetitprince_chap1.txt') as f:\n",
        "    sentences = f.readlines()"
      ],
      "metadata": {
        "id": "tsw6rkteKky7"
      },
      "id": "tsw6rkteKky7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ot2tF6FsWRJ0"
      },
      "source": [
        "## Partie I. *Word embeddings* ou la mod√©lisation des mots\n",
        "\n",
        "Nous utiliserons les libraries *nltk* et *gensim* pour cr√©er des mod√®les *word2vec*\n",
        "\n",
        "* I.1) Convertissez le texte (phrases) en tokens\n",
        "* I.2) Construisez un mod√®le *CBOW* et un *skip-gram* avec le corpus du livre\n"
      ],
      "id": "ot2tF6FsWRJ0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wRDkO5zWRJ3"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "#Ecrivez votre code ici\n"
      ],
      "id": "3wRDkO5zWRJ3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "* I.3) Quel est le vecteur associ√© au mot \"boa\" ? Est-il le m√™me dans les deux mod√®les (*CBOW* et *skip-gram*) ?\n",
        "* I.4) Quels mots sont \"similaires\" au mot \"boa\" ? Sont-ils les m√™mes dans les deux mod√®les ?"
      ],
      "metadata": {
        "id": "rPYo_H8FNB_S"
      },
      "id": "rPYo_H8FNB_S"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5wcNSeVWRKA"
      },
      "outputs": [],
      "source": [
        "#Ecrivez votre code ici\n"
      ],
      "id": "I5wcNSeVWRKA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "* I.5) Affichez les graphiques des deux mod√®les en faisant une r√©duction de dimension √† 2D par t-SNE (voir code d'exemple ci-dessous). Quelles sont les coordonn√©es en 2D du mot \"boa\" dans les deux mod√®les ?"
      ],
      "metadata": {
        "id": "JMK48xrNNe-A"
      },
      "id": "JMK48xrNNe-A"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Affectez le nom de votre mod√®le Word2Vec √† la variable model_w2v ci-dessous\n",
        "model_w2v = #votre_model_w2v\n",
        "\n",
        "# t-SNE pour r√©duire √† 2 dimensions\n",
        "word_vectors = model_w2v.wv.vectors\n",
        "num_samples = len(word_vectors)\n",
        "tsne_perplexity = min(30, num_samples - 1)  # Perplexity must be less than the number of samples\n",
        "\n",
        "tsne = TSNE(n_components=2, perplexity=tsne_perplexity, random_state=0)\n",
        "word_vectors_2d = tsne.fit_transform(word_vectors)\n",
        "\n",
        "# Plot the vectors\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i, word in enumerate(model_w2v.wv.index_to_key):\n",
        "    plt.scatter(word_vectors_2d[i, 0], word_vectors_2d[i, 1])\n",
        "    plt.annotate(word, xy=(word_vectors_2d[i, 0], word_vectors_2d[i, 1]), xytext=(5, 2),\n",
        "                 textcoords='offset points', ha='right', va='bottom')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VHDli7m-ott4"
      },
      "id": "VHDli7m-ott4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1IbednbWRKI"
      },
      "source": [
        "## Partie II. R√©seaux de neurones et attention\n",
        "\n",
        "* II.1) Construisez et entra√Ænez un r√©seau de neurones dense pour pr√©dire le cinqu√®me token de chaque phrase en fonction des quatre premiers tokens d'un mod√®le Word2Vec. Quelle est l'erreur d'apprentissage ?\n",
        "* II.2) Quelle est la dimension de la couche d'entr√©e ?\n",
        "* II.3) Quelle est la dimension de la couche de sortie ?\n",
        "* II.4) Quelle est l'erreur RMS pour la 10√®me phrase ?\n"
      ],
      "id": "J1IbednbWRKI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SI7tyTgwWRKU"
      },
      "outputs": [],
      "source": [
        "#Ecrivez votre code ici\n"
      ],
      "id": "SI7tyTgwWRKU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxRPx84AWRKZ"
      },
      "source": [
        "## Partie III. Transformers\n",
        "\n",
        "\n",
        "Pour cette partie, nous utiliserons le mod√®le *CamemBERT*, qui est mod√®le type BERT (Transformer) sp√©cifique √† la langue fran√ßaise. Ce mod√®le est disponible dans la librairie *Transformers* de *HuggingFace* (documentation disponible sur : https://huggingface.co/docs/transformers/model_doc/camembert). Nous travaillerons le fichier 'lepetitprince_chap1_masked.txt', qui contient des mots masqu√©s.\n",
        "\n",
        "* III.1) \"Tokenisez\" le texte. Quelle est la dimension du vecteur de mod√©lisation des tokens ?"
      ],
      "id": "HxRPx84AWRKZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DUG0ViJWRKa"
      },
      "outputs": [],
      "source": [
        "#Ecrivez votre code ici\n"
      ],
      "id": "6DUG0ViJWRKa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "* III.2) Pr√©disez les mots masqu√©s. Donnez les 3 mots les plus probables pour les 10 premi√®res phrases."
      ],
      "metadata": {
        "id": "w80QaU47TB5w"
      },
      "id": "w80QaU47TB5w"
    },
    {
      "cell_type": "code",
      "source": [
        "#Ecrivez votre code ici\n"
      ],
      "metadata": {
        "id": "5JuTieEi9JWv"
      },
      "id": "5JuTieEi9JWv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Partie IV. LLM\n",
        "\n",
        "* IV.I) R√©alisez √† nouveau la partie III en utilisant cette fois-ci Llama2 ou Mixtral (mod√®les disponibles dans la librairie *Transformers* de *HuggingFace*). Llama2 est aussi t√©l√©chargeable sur https://llama-2.ai/download/\n",
        "* IV.2) Faites un r√©sum√© automatique en 20 mots environ du chapitre I"
      ],
      "metadata": {
        "id": "HLSWnM3FpOv5"
      },
      "id": "HLSWnM3FpOv5"
    },
    {
      "cell_type": "code",
      "source": [
        "#Ecrivez votre code ici\n"
      ],
      "metadata": {
        "id": "83QEn5Phpmxi"
      },
      "id": "83QEn5Phpmxi",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "jupytext": {
      "main_language": "python"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "nbreset": "https://raw.githubusercontent.com/INRIA/scikit-learn-mooc/main/notebooks/01_tabular_data_exploration_ex_01.ipynb",
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}